# Yaml file to look for the prompts
test_file: "test_prompts.yaml"
# Number of times to test with each prompt
test_runs: 5
# Flag to use Ollama defualt volume for speed up if set True
ollama_vol_flag: True
# Flag to automatical remove ollama volume after run
remove_ollama_vol_flag: False
# List of models to try in ASCENDING SIZE
model_list: ['llama3.1:8b-instruct-q4_0', 'llama3.1:8b-instruct-q8_0', 'llama3.1:8b-instruct-fp16']
# List of GPU_IDs to test
gpu_id_lists: [
  [cpu]
]
